{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2464d03d",
   "metadata": {
    "papermill": {
     "duration": 0.008061,
     "end_time": "2022-11-04T02:50:48.550678",
     "exception": false,
     "start_time": "2022-11-04T02:50:48.542617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RAPIDS SVR - CV 0.450 - LB 0.44x !!\n",
    "\n",
    "This notebook uses code and ideas from Noufal's great notebook [here][1]. In his notebook he extracts 1 NLP transformer embeddings and trains Sklearn's multioutput regressor + gradient boosting regressor on CPU with 5-Folds.\n",
    "\n",
    "In this notebook, we use [RAPIDS SVR][3] to train and predict. Since [RAPIDS cuML's SVR][3] uses GPU it is very fast. This allows us to train with more extracted embeddings quickly and more folds. In this notebook, we use 25-Folds! And in this notebook, we extract embeddings from 5 NLP transformers. Afterward we concatenate them and have 6000 columns of features! [RAPIDS SVR][3] has built in feature reduction, so it learns to use the most informative features from all the NLP transformers!\n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Sep-2022/svr.png)\n",
    "\n",
    "Note that we do not finetune the NLP transformers. The Deberta transformers in this notebook are the same pretrained transformers that we download from Hugging Face. They have not been finetuned on Kaggle's competition data. This demonstrates that pretrained models already come with intelligence.\n",
    "\n",
    "This is similar to Giba's 1st place solution in Kaggle's Pet Competition [here][2]. That competition was computer vision regression. Giba extracted embeddings from dozens of image CNN's and image transformers. The models were pretrained (most likely on ImageNet data) but not finetuned (on Kaggle competition data). He concatenated the embeddings and trained a [RAPIDS SVR][3] on tens of thousands of feature columns!\n",
    "\n",
    "[1]: https://www.kaggle.com/code/kvsnoufal/lb0-46-gb-debertaembedding\n",
    "[2]: https://www.kaggle.com/competitions/petfinder-pawpularity-score/discussion/301686\n",
    "[3]: https://docs.rapids.ai/api/cuml/stable/api.html#support-vector-machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f66c8",
   "metadata": {
    "papermill": {
     "duration": 0.00636,
     "end_time": "2022-11-04T02:50:48.563790",
     "exception": false,
     "start_time": "2022-11-04T02:50:48.557430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62289da1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:48.579239Z",
     "iopub.status.busy": "2022-11-04T02:50:48.578452Z",
     "iopub.status.idle": "2022-11-04T02:50:54.653483Z",
     "shell.execute_reply": "2022-11-04T02:50:54.652451Z"
    },
    "papermill": {
     "duration": 6.085609,
     "end_time": "2022-11-04T02:50:54.656128",
     "exception": false,
     "start_time": "2022-11-04T02:50:48.570519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAPIDS version 21.10.02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, gc, re, warnings\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from cuml.svm import SVR\n",
    "import cuml\n",
    "print('RAPIDS version',cuml.__version__)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549affcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:54.671387Z",
     "iopub.status.busy": "2022-11-04T02:50:54.670423Z",
     "iopub.status.idle": "2022-11-04T02:50:54.920866Z",
     "shell.execute_reply": "2022-11-04T02:50:54.919868Z"
    },
    "papermill": {
     "duration": 0.260459,
     "end_time": "2022-11-04T02:50:54.923363",
     "exception": false,
     "start_time": "2022-11-04T02:50:54.662904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3911, 9) Test shape: (3, 3) Test columns: Index(['text_id', 'full_text', 'src'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions    src  \n",
       "0     3.5         3.0          3.0      4.0          3.0  train  \n",
       "1     2.5         3.0          2.0      2.0          2.5  train  \n",
       "2     3.5         3.0          3.0      3.0          2.5  train  \n",
       "3     4.5         4.5          4.5      4.0          5.0  train  \n",
       "4     3.0         3.0          3.0      2.5          2.5  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftr = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/train.csv\")\n",
    "dftr[\"src\"]=\"train\"\n",
    "dfte = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/test.csv\")\n",
    "dfte[\"src\"]=\"test\"\n",
    "print('Train shape:',dftr.shape,'Test shape:',dfte.shape,'Test columns:',dfte.columns)\n",
    "df = pd.concat([dftr,dfte],ignore_index=True)\n",
    "\n",
    "dftr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0133c184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:54.938502Z",
     "iopub.status.busy": "2022-11-04T02:50:54.938216Z",
     "iopub.status.idle": "2022-11-04T02:50:55.152454Z",
     "shell.execute_reply": "2022-11-04T02:50:55.151485Z"
    },
    "papermill": {
     "duration": 0.224094,
     "end_time": "2022-11-04T02:50:55.154601",
     "exception": false,
     "start_time": "2022-11-04T02:50:54.930507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think that students would benefit from memor...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear, Principal If u change the school policy ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Small human action of kindness can impact in o...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  cohesion  syntax  \\\n",
       "0  I think that students would benefit from memor...       3.5     3.5   \n",
       "1  When a problem is a change you have to let it ...       2.5     2.5   \n",
       "2  Dear, Principal If u change the school policy ...       3.0     3.5   \n",
       "3  The best time in life is when you become yours...       4.5     4.5   \n",
       "4  Small human action of kindness can impact in o...       2.5     3.0   \n",
       "\n",
       "   vocabulary  phraseology  grammar  conventions    src  \n",
       "0         3.0          3.0      4.0          3.0  train  \n",
       "1         3.0          2.0      2.0          2.5  train  \n",
       "2         3.0          3.0      3.0          2.5  train  \n",
       "3         4.5          4.5      4.0          5.0  train  \n",
       "4         3.0          3.0      2.5          2.5  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_aug = pd.read_csv('../input/final-aug/syn_aug.csv')\n",
    "syn_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e659ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:55.172717Z",
     "iopub.status.busy": "2022-11-04T02:50:55.171261Z",
     "iopub.status.idle": "2022-11-04T02:50:55.176830Z",
     "shell.execute_reply": "2022-11-04T02:50:55.175908Z"
    },
    "papermill": {
     "duration": 0.015833,
     "end_time": "2022-11-04T02:50:55.178987",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.163154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dftr = pd.concat([dftr,syn_aug], axis = 0)\n",
    "# dftr.reset_index(inplace=True, drop=True) \n",
    "# dftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb45a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:55.194607Z",
     "iopub.status.busy": "2022-11-04T02:50:55.194313Z",
     "iopub.status.idle": "2022-11-04T02:50:55.199021Z",
     "shell.execute_reply": "2022-11-04T02:50:55.197964Z"
    },
    "papermill": {
     "duration": 0.015461,
     "end_time": "2022-11-04T02:50:55.201433",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.185972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions',]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d0a5a",
   "metadata": {
    "papermill": {
     "duration": 0.006801,
     "end_time": "2022-11-04T02:50:55.216192",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.209391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make 25 Stratified Folds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f541cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:55.231267Z",
     "iopub.status.busy": "2022-11-04T02:50:55.230984Z",
     "iopub.status.idle": "2022-11-04T02:50:55.409493Z",
     "shell.execute_reply": "2022-11-04T02:50:55.407696Z"
    },
    "papermill": {
     "duration": 0.18927,
     "end_time": "2022-11-04T02:50:55.412366",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.223096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples per fold:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.0    157\n",
       "21.0    157\n",
       "1.0     157\n",
       "7.0     157\n",
       "20.0    157\n",
       "14.0    157\n",
       "19.0    157\n",
       "12.0    157\n",
       "23.0    157\n",
       "3.0     157\n",
       "6.0     157\n",
       "5.0     156\n",
       "13.0    156\n",
       "22.0    156\n",
       "0.0     156\n",
       "15.0    156\n",
       "24.0    156\n",
       "17.0    156\n",
       "9.0     156\n",
       "8.0     156\n",
       "4.0     156\n",
       "2.0     156\n",
       "18.0    156\n",
       "10.0    156\n",
       "16.0    156\n",
       "Name: FOLD, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "FOLDS = 25\n",
    "skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for i,(train_index, val_index) in enumerate(skf.split(dftr,dftr[target_cols])):\n",
    "    dftr.loc[val_index,'FOLD'] = i\n",
    "print('Train samples per fold:')\n",
    "dftr.FOLD.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d556721",
   "metadata": {
    "papermill": {
     "duration": 0.007028,
     "end_time": "2022-11-04T02:50:55.427124",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.420096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ecb086",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:55.443454Z",
     "iopub.status.busy": "2022-11-04T02:50:55.442639Z",
     "iopub.status.idle": "2022-11-04T02:50:55.448311Z",
     "shell.execute_reply": "2022-11-04T02:50:55.447426Z"
    },
    "papermill": {
     "duration": 0.015721,
     "end_time": "2022-11-04T02:50:55.450319",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.434598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state.detach().cpu()\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16020c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:55.467203Z",
     "iopub.status.busy": "2022-11-04T02:50:55.465624Z",
     "iopub.status.idle": "2022-11-04T02:50:55.476152Z",
     "shell.execute_reply": "2022-11-04T02:50:55.475356Z"
    },
    "papermill": {
     "duration": 0.020746,
     "end_time": "2022-11-04T02:50:55.478123",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.457377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "class EmbedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx,\"full_text\"]\n",
    "        tokens = tokenizer(\n",
    "                text,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=MAX_LEN,return_tensors=\"pt\")\n",
    "        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "ds_tr = EmbedDataset(dftr)\n",
    "embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)\n",
    "ds_te = EmbedDataset(dfte)\n",
    "embed_dataloader_te = torch.utils.data.DataLoader(ds_te,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)\n",
    "\n",
    "ds_syn_aug = EmbedDataset(syn_aug)\n",
    "embed_dataloader_syn_aug = torch.utils.data.DataLoader(ds_syn_aug,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d0ed9",
   "metadata": {
    "papermill": {
     "duration": 0.006899,
     "end_time": "2022-11-04T02:50:55.492110",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.485211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec2e7aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:55.507894Z",
     "iopub.status.busy": "2022-11-04T02:50:55.507227Z",
     "iopub.status.idle": "2022-11-04T02:50:55.516091Z",
     "shell.execute_reply": "2022-11-04T02:50:55.515240Z"
    },
    "papermill": {
     "duration": 0.01884,
     "end_time": "2022-11-04T02:50:55.518082",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.499242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "MAX_LEN = 640\n",
    "\n",
    "def get_embeddings(embed_dataloader, MODEL_NM='', MAX=640, BATCH_SIZE=4, verbose=True):\n",
    "    global tokenizer, MAX_LEN\n",
    "    DEVICE=\"cuda\"\n",
    "    model = AutoModel.from_pretrained( MODEL_NM )\n",
    "    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n",
    "    MAX_LEN = MAX\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    text_feats = []\n",
    "    for batch in tqdm(embed_dataloader,total=len(embed_dataloader)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        text_feats.extend(sentence_embeddings)\n",
    "    text_feats = np.array(text_feats)\n",
    "    if verbose:\n",
    "        print('Embeddings shape',text_feats.shape)\n",
    "        \n",
    "#     te_text_feats = []\n",
    "#     for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n",
    "#         input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "#         with torch.no_grad():\n",
    "#             model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "#         sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "#         # Normalize the embeddings\n",
    "#         sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "#         sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "#         te_text_feats.extend(sentence_embeddings)\n",
    "#     te_text_feats = np.array(te_text_feats)\n",
    "#     if verbose:\n",
    "#         print('Test embeddings shape',te_text_feats.shape)\n",
    "        \n",
    "    return text_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bb34e",
   "metadata": {
    "papermill": {
     "duration": 0.007168,
     "end_time": "2022-11-04T02:50:55.532296",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.525128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "042934bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:50:55.547904Z",
     "iopub.status.busy": "2022-11-04T02:50:55.547234Z",
     "iopub.status.idle": "2022-11-04T02:57:19.078490Z",
     "shell.execute_reply": "2022-11-04T02:57:19.077349Z"
    },
    "papermill": {
     "duration": 383.541334,
     "end_time": "2022-11-04T02:57:19.080688",
     "exception": false,
     "start_time": "2022-11-04T02:50:55.539354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-base/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 978/978 [03:02<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-base/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-base/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 978/978 [03:01<00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = '../input/huggingface-deberta-variants/deberta-base/deberta-base'\n",
    "all_train_text_feats = get_embeddings(embed_dataloader_tr, MODEL_NM)\n",
    "te_text_feats = get_embeddings(embed_dataloader_te, MODEL_NM)\n",
    "aug_text_feats = get_embeddings(embed_dataloader_syn_aug, MODEL_NM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf6871",
   "metadata": {
    "papermill": {
     "duration": 0.101733,
     "end_time": "2022-11-04T02:57:19.285220",
     "exception": false,
     "start_time": "2022-11-04T02:57:19.183487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get Large V3 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2958fe7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T02:57:19.510758Z",
     "iopub.status.busy": "2022-11-04T02:57:19.510379Z",
     "iopub.status.idle": "2022-11-04T03:13:55.944855Z",
     "shell.execute_reply": "2022-11-04T03:13:55.943632Z"
    },
    "papermill": {
     "duration": 996.542686,
     "end_time": "2022-11-04T03:13:55.947667",
     "exception": false,
     "start_time": "2022-11-04T02:57:19.404981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [08:05<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [08:06<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = '../input/deberta-v3-large/deberta-v3-large'\n",
    "all_train_text_feats2 = get_embeddings(embed_dataloader_tr, MODEL_NM)\n",
    "te_text_feats2 = get_embeddings(embed_dataloader_te, MODEL_NM)\n",
    "aug_text_feats2 = get_embeddings(embed_dataloader_syn_aug, MODEL_NM)\n",
    "\n",
    "all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2],axis=1)\n",
    "\n",
    "te_text_feats = np.concatenate([te_text_feats,te_text_feats2],axis=1)\n",
    "\n",
    "aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07642913",
   "metadata": {
    "papermill": {
     "duration": 0.203445,
     "end_time": "2022-11-04T03:13:56.413619",
     "exception": false,
     "start_time": "2022-11-04T03:13:56.210174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get Large Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda605d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T03:13:56.808450Z",
     "iopub.status.busy": "2022-11-04T03:13:56.808079Z",
     "iopub.status.idle": "2022-11-04T03:30:37.451137Z",
     "shell.execute_reply": "2022-11-04T03:30:37.449946Z"
    },
    "papermill": {
     "duration": 1000.844394,
     "end_time": "2022-11-04T03:30:37.453503",
     "exception": false,
     "start_time": "2022-11-04T03:13:56.609109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-large/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 978/978 [08:05<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-large/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-large/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 978/978 [08:07<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = '../input/huggingface-deberta-variants/deberta-large/deberta-large'\n",
    "all_train_text_feats2 = get_embeddings(embed_dataloader_tr, MODEL_NM)\n",
    "te_text_feats2 = get_embeddings(embed_dataloader_te, MODEL_NM)\n",
    "aug_text_feats2 = get_embeddings(embed_dataloader_syn_aug, MODEL_NM)\n",
    "\n",
    "all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2],axis=1)\n",
    "\n",
    "te_text_feats = np.concatenate([te_text_feats,te_text_feats2],axis=1)\n",
    "\n",
    "aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71dfbc",
   "metadata": {
    "papermill": {
     "duration": 0.290277,
     "end_time": "2022-11-04T03:30:38.083241",
     "exception": false,
     "start_time": "2022-11-04T03:30:37.792964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get Large MNLI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fcf43af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T03:30:38.887712Z",
     "iopub.status.busy": "2022-11-04T03:30:38.887214Z",
     "iopub.status.idle": "2022-11-04T03:43:47.163795Z",
     "shell.execute_reply": "2022-11-04T03:43:47.162731Z"
    },
    "papermill": {
     "duration": 788.750663,
     "end_time": "2022-11-04T03:43:47.166466",
     "exception": false,
     "start_time": "2022-11-04T03:30:38.415803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli were not used when initializing DebertaModel: ['pooler.dense.weight', 'pooler.dense.bias', 'config', 'classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 978/978 [06:21<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli were not used when initializing DebertaModel: ['pooler.dense.weight', 'pooler.dense.bias', 'config', 'classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli were not used when initializing DebertaModel: ['pooler.dense.weight', 'pooler.dense.bias', 'config', 'classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 978/978 [06:20<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = '../input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli'\n",
    "all_train_text_feats2 = get_embeddings(embed_dataloader_tr, MODEL_NM, MAX=512)\n",
    "te_text_feats2 = get_embeddings(embed_dataloader_te, MODEL_NM, MAX=512)\n",
    "aug_text_feats2 = get_embeddings(embed_dataloader_syn_aug, MODEL_NM, MAX=512)\n",
    "\n",
    "all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2],axis=1)\n",
    "\n",
    "te_text_feats = np.concatenate([te_text_feats,te_text_feats2],axis=1)\n",
    "\n",
    "aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e298fc",
   "metadata": {
    "papermill": {
     "duration": 0.442405,
     "end_time": "2022-11-04T03:43:48.022994",
     "exception": false,
     "start_time": "2022-11-04T03:43:47.580589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get XLarge Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f56836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T03:43:48.848993Z",
     "iopub.status.busy": "2022-11-04T03:43:48.848592Z",
     "iopub.status.idle": "2022-11-04T04:09:23.471647Z",
     "shell.execute_reply": "2022-11-04T04:09:23.469440Z"
    },
    "papermill": {
     "duration": 1535.048706,
     "end_time": "2022-11-04T04:09:23.473711",
     "exception": false,
     "start_time": "2022-11-04T03:43:48.425005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 978/978 [12:23<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 978/978 [12:23<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = '../input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge'\n",
    "all_train_text_feats2 = get_embeddings(embed_dataloader_tr, MODEL_NM, MAX=512)\n",
    "te_text_feats2 = get_embeddings(embed_dataloader_te, MODEL_NM, MAX=512)\n",
    "aug_text_feats2 = get_embeddings(embed_dataloader_syn_aug, MODEL_NM, MAX=512)\n",
    "\n",
    "all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2],axis=1)\n",
    "\n",
    "te_text_feats = np.concatenate([te_text_feats,te_text_feats2],axis=1)\n",
    "\n",
    "aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1306167d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T04:09:24.464771Z",
     "iopub.status.busy": "2022-11-04T04:09:24.463387Z",
     "iopub.status.idle": "2022-11-04T04:16:04.618655Z",
     "shell.execute_reply": "2022-11-04T04:16:04.617555Z"
    },
    "papermill": {
     "duration": 400.647445,
     "end_time": "2022-11-04T04:16:04.622422",
     "exception": false,
     "start_time": "2022-11-04T04:09:23.974977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [03:13<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [03:13<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 768)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = '../input/debertav3base'\n",
    "all_train_text_feats2 = get_embeddings(embed_dataloader_tr, MODEL_NM)\n",
    "te_text_feats2 = get_embeddings(embed_dataloader_te, MODEL_NM)\n",
    "aug_text_feats2 = get_embeddings(embed_dataloader_syn_aug, MODEL_NM)\n",
    "\n",
    "all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2],axis=1)\n",
    "\n",
    "te_text_feats = np.concatenate([te_text_feats,te_text_feats2],axis=1)\n",
    "\n",
    "aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eba13102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T04:16:05.859883Z",
     "iopub.status.busy": "2022-11-04T04:16:05.858811Z",
     "iopub.status.idle": "2022-11-04T04:19:58.004930Z",
     "shell.execute_reply": "2022-11-04T04:19:58.003880Z"
    },
    "papermill": {
     "duration": 232.791298,
     "end_time": "2022-11-04T04:19:58.007605",
     "exception": false,
     "start_time": "2022-11-04T04:16:05.216307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/debertav3small were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [01:50<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/debertav3small were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/debertav3small were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [01:50<00:00,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = '../input/debertav3small'\n",
    "all_train_text_feats2 = get_embeddings(embed_dataloader_tr, MODEL_NM)\n",
    "te_text_feats2 = get_embeddings(embed_dataloader_te, MODEL_NM)\n",
    "aug_text_feats2 = get_embeddings(embed_dataloader_syn_aug, MODEL_NM)\n",
    "\n",
    "\n",
    "all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2],axis=1)\n",
    "\n",
    "te_text_feats = np.concatenate([te_text_feats,te_text_feats2],axis=1)\n",
    "\n",
    "aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e39956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T04:19:59.472197Z",
     "iopub.status.busy": "2022-11-04T04:19:59.471735Z",
     "iopub.status.idle": "2022-11-04T04:42:28.694425Z",
     "shell.execute_reply": "2022-11-04T04:42:28.693307Z"
    },
    "papermill": {
     "duration": 1349.993061,
     "end_time": "2022-11-04T04:42:28.698641",
     "exception": false,
     "start_time": "2022-11-04T04:19:58.705580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [10:48<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [10:41<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = '../input/deberta-v2-xlarge'\n",
    "all_train_text_feats2 = get_embeddings(embed_dataloader_tr, MODEL_NM, MAX=512)\n",
    "te_text_feats2 = get_embeddings(embed_dataloader_te, MODEL_NM, MAX=512)\n",
    "aug_text_feats2 = get_embeddings(embed_dataloader_syn_aug, MODEL_NM, MAX=512)\n",
    "\n",
    "\n",
    "all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2],axis=1)\n",
    "\n",
    "te_text_feats = np.concatenate([te_text_feats,te_text_feats2],axis=1)\n",
    "\n",
    "aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd125286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T04:42:30.427085Z",
     "iopub.status.busy": "2022-11-04T04:42:30.426639Z",
     "iopub.status.idle": "2022-11-04T05:26:32.484743Z",
     "shell.execute_reply": "2022-11-04T05:26:32.483166Z"
    },
    "papermill": {
     "duration": 2642.90916,
     "end_time": "2022-11-04T05:26:32.488975",
     "exception": false,
     "start_time": "2022-11-04T04:42:29.579815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v2-xxlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [20:51<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v2-xxlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v2-xxlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 978/978 [20:51<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (3911, 1536)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NM = '../input/deberta-v2-xxlarge'\n",
    "all_train_text_feats2 = get_embeddings(embed_dataloader_tr, MODEL_NM, MAX=512)\n",
    "te_text_feats2 = get_embeddings(embed_dataloader_te, MODEL_NM, MAX=512)\n",
    "aug_text_feats2 = get_embeddings(embed_dataloader_syn_aug, MODEL_NM, MAX=512)\n",
    "\n",
    "\n",
    "all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2],axis=1)\n",
    "\n",
    "te_text_feats = np.concatenate([te_text_feats,te_text_feats2],axis=1)\n",
    "\n",
    "aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2],axis=1)\n",
    "\n",
    "del all_train_text_feats2, te_text_feats2, aug_text_feats2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ab233",
   "metadata": {
    "papermill": {
     "duration": 0.944302,
     "end_time": "2022-11-04T05:26:34.318445",
     "exception": false,
     "start_time": "2022-11-04T05:26:33.374143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Combine Feature Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e160d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T05:26:36.140516Z",
     "iopub.status.busy": "2022-11-04T05:26:36.140161Z",
     "iopub.status.idle": "2022-11-04T05:26:36.145642Z",
     "shell.execute_reply": "2022-11-04T05:26:36.144574Z"
    },
    "papermill": {
     "duration": 0.948933,
     "end_time": "2022-11-04T05:26:36.147618",
     "exception": false,
     "start_time": "2022-11-04T05:26:35.198685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2,\n",
    "#                                        all_train_text_feats3,all_train_text_feats4,\n",
    "#                                        all_train_text_feats5,all_train_text_feats6,\n",
    "#                                       all_train_text_feats7,all_train_text_feats8,\n",
    "#                                       all_train_text_feats9],axis=1)\n",
    "\n",
    "# te_text_feats = np.concatenate([te_text_feats,te_text_feats2,\n",
    "#                                 te_text_feats3,te_text_feats4,\n",
    "#                                 te_text_feats5,te_text_feats6,\n",
    "#                                te_text_feats7,te_text_feats8,\n",
    "#                                te_text_feats9],axis=1)\n",
    "\n",
    "# aug_text_feats = np.concatenate([aug_text_feats,aug_text_feats2,\n",
    "#                                 aug_text_feats3,aug_text_feats4,\n",
    "#                                 aug_text_feats5,aug_text_feats6,\n",
    "#                                 aug_text_feats7,aug_text_feats8,\n",
    "#                                 aug_text_feats9],axis=1)\n",
    "\n",
    "# del all_train_text_feats2, te_text_feats2, aug_text_feats2\n",
    "# del all_train_text_feats3, te_text_feats3, aug_text_feats3\n",
    "# del all_train_text_feats4, te_text_feats4, aug_text_feats4\n",
    "# del all_train_text_feats5, te_text_feats5, aug_text_feats5\n",
    "# del all_train_text_feats6, te_text_feats6, aug_text_feats6\n",
    "# del all_train_text_feats7, te_text_feats7, aug_text_feats7\n",
    "# del all_train_text_feats8, te_text_feats8, aug_text_feats8\n",
    "# del all_train_text_feats9, te_text_feats9, aug_text_feats9\n",
    "# gc.collect()\n",
    "\n",
    "# print('Our concatenated embeddings have shape', all_train_text_feats.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8bb6e",
   "metadata": {
    "papermill": {
     "duration": 0.872621,
     "end_time": "2022-11-04T05:26:37.965173",
     "exception": false,
     "start_time": "2022-11-04T05:26:37.092552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train RAPIDS cuML SVR\n",
    "Documentation for RAPIDS SVM is [here][1]\n",
    "\n",
    "[1]: https://docs.rapids.ai/api/cuml/stable/api.html#support-vector-machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a332f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T05:26:40.260255Z",
     "iopub.status.busy": "2022-11-04T05:26:40.259906Z",
     "iopub.status.idle": "2022-11-04T05:28:56.748422Z",
     "shell.execute_reply": "2022-11-04T05:28:56.747181Z"
    },
    "papermill": {
     "duration": 137.412491,
     "end_time": "2022-11-04T05:28:56.750948",
     "exception": false,
     "start_time": "2022-11-04T05:26:39.338457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 0 RSME score: 0.4591829711255742\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 1 RSME score: 0.4488037060577386\n",
      "#########################\n",
      "### Fold 3\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 2 RSME score: 0.4587086124034481\n",
      "#########################\n",
      "### Fold 4\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 3 RSME score: 0.4483499211128675\n",
      "#########################\n",
      "### Fold 5\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 4 RSME score: 0.44925653433853885\n",
      "#########################\n",
      "### Fold 6\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 5 RSME score: 0.43982219929000693\n",
      "#########################\n",
      "### Fold 7\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 6 RSME score: 0.4390507209167205\n",
      "#########################\n",
      "### Fold 8\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 7 RSME score: 0.46355126613364267\n",
      "#########################\n",
      "### Fold 9\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 8 RSME score: 0.4372315843245393\n",
      "#########################\n",
      "### Fold 10\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 9 RSME score: 0.4799768255179881\n",
      "#########################\n",
      "### Fold 11\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 10 RSME score: 0.4428800539053807\n",
      "#########################\n",
      "### Fold 12\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 11 RSME score: 0.4425409805804415\n",
      "#########################\n",
      "### Fold 13\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 12 RSME score: 0.46123791983347856\n",
      "#########################\n",
      "### Fold 14\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 13 RSME score: 0.4298139476708465\n",
      "#########################\n",
      "### Fold 15\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 14 RSME score: 0.4630559175213964\n",
      "#########################\n",
      "### Fold 16\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 15 RSME score: 0.4340268704840151\n",
      "#########################\n",
      "### Fold 17\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 16 RSME score: 0.42875934733194826\n",
      "#########################\n",
      "### Fold 18\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 17 RSME score: 0.4631415432687927\n",
      "#########################\n",
      "### Fold 19\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 18 RSME score: 0.4601798318710006\n",
      "#########################\n",
      "### Fold 20\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 19 RSME score: 0.43144515941054024\n",
      "#########################\n",
      "### Fold 21\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 20 RSME score: 0.4655280247067739\n",
      "#########################\n",
      "### Fold 22\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 21 RSME score: 0.46038460724828756\n",
      "#########################\n",
      "### Fold 23\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 22 RSME score: 0.4531422474968693\n",
      "#########################\n",
      "### Fold 24\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 23 RSME score: 0.45453941035265105\n",
      "#########################\n",
      "### Fold 25\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 24 RSME score: 0.4321348041617524\n",
      "#########################\n",
      "Overall CV RSME = 0.4498698002826096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = []\n",
    "scores = []\n",
    "def comp_score(y_true,y_pred):\n",
    "    rmse_scores = []\n",
    "    for i in range(len(target_cols)):\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "#for fold in tqdm(range(FOLDS),total=FOLDS):\n",
    "for fold in range(FOLDS):\n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('#'*25)\n",
    "    \n",
    "    dftr_ = dftr[dftr[\"FOLD\"]!=fold]\n",
    "    dfev_ = dftr[dftr[\"FOLD\"]==fold]\n",
    "#     print(dftr_.isnull().sum())\n",
    "    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n",
    "    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n",
    "    aug_text_train_feats = aug_text_feats[list(dftr_.index),:]\n",
    "    \n",
    "    aug_df = syn_aug.iloc[list(dftr_.index)].reset_index(drop=True)\n",
    "    \n",
    "#     aug_text_feats_df = pd.DataFrame(aug_text_train_feats).reset_index(drop=True)\n",
    "#     dftr_text_feats_df = pd.DataFrame(tr_text_feats).reset_index(drop=True)\n",
    "#     print(dftr_text_feats_df.isnull().sum())\n",
    "#     dftr_ = pd.concat([dftr_,dftr_text_feats_df], axis = 1)\n",
    "#     print(dftr_.isnull().sum())\n",
    "#     aug_df = pd.concat([aug_df,aug_text_feats_df], axis = 1)\n",
    "    \n",
    "    dftr_ = pd.concat([dftr_, aug_df],axis=0).reset_index(drop=True)\n",
    "    tr_text_feats = np.concatenate((tr_text_feats,aug_text_train_feats),axis=0)\n",
    "\n",
    "#     dftr_ = dftr_.sample(frac=1)\n",
    "#     dftr_.reset_index(inplace=True, drop=True) \n",
    "#     print('      ')\n",
    "#     print(dftr_.iloc[:,10:].isnull().sum())\n",
    "    \n",
    "    ev_preds = np.zeros((len(ev_text_feats),6))\n",
    "    test_preds = np.zeros((len(te_text_feats),6))\n",
    "    for i,t in enumerate(target_cols):\n",
    "        print(t,', ',end='')\n",
    "        clf = SVR(C=1)\n",
    "        clf.fit(tr_text_feats, dftr_[t].values)\n",
    "        ev_preds[:,i] = clf.predict(ev_text_feats)\n",
    "        test_preds[:,i] = clf.predict(te_text_feats)\n",
    "    print()\n",
    "    score = comp_score(dfev_[target_cols].values,ev_preds)\n",
    "    scores.append(score)\n",
    "    print(\"Fold : {} RSME score: {}\".format(fold,score))\n",
    "    preds.append(test_preds)\n",
    "    \n",
    "print('#'*25)\n",
    "print('Overall CV RSME =',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c2744",
   "metadata": {
    "papermill": {
     "duration": 0.885431,
     "end_time": "2022-11-04T05:29:00.448380",
     "exception": false,
     "start_time": "2022-11-04T05:28:59.562949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee389f57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T05:29:02.282678Z",
     "iopub.status.busy": "2022-11-04T05:29:02.282293Z",
     "iopub.status.idle": "2022-11-04T05:29:02.321152Z",
     "shell.execute_reply": "2022-11-04T05:29:02.320286Z"
    },
    "papermill": {
     "duration": 0.994817,
     "end_time": "2022-11-04T05:29:02.323239",
     "exception": false,
     "start_time": "2022-11-04T05:29:01.328422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = dfte.copy()\n",
    "\n",
    "sub.loc[:,target_cols] = np.average(np.array(preds),axis=0) #,weights=[1/s for s in scores]\n",
    "sub_columns = pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\").columns\n",
    "sub = sub[sub_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5d227e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T05:29:04.170466Z",
     "iopub.status.busy": "2022-11-04T05:29:04.170097Z",
     "iopub.status.idle": "2022-11-04T05:29:04.191096Z",
     "shell.execute_reply": "2022-11-04T05:29:04.190113Z"
    },
    "papermill": {
     "duration": 0.9172,
     "end_time": "2022-11-04T05:29:04.193196",
     "exception": false,
     "start_time": "2022-11-04T05:29:03.275996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>2.889608</td>\n",
       "      <td>2.803495</td>\n",
       "      <td>3.095255</td>\n",
       "      <td>2.934777</td>\n",
       "      <td>2.680838</td>\n",
       "      <td>2.623977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>2.659902</td>\n",
       "      <td>2.452345</td>\n",
       "      <td>2.705822</td>\n",
       "      <td>2.283923</td>\n",
       "      <td>2.047846</td>\n",
       "      <td>2.704002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.664221</td>\n",
       "      <td>3.448133</td>\n",
       "      <td>3.578464</td>\n",
       "      <td>3.666472</td>\n",
       "      <td>3.410824</td>\n",
       "      <td>3.292725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0  0000C359D63E  2.889608  2.803495    3.095255     2.934777  2.680838   \n",
       "1  000BAD50D026  2.659902  2.452345    2.705822     2.283923  2.047846   \n",
       "2  00367BB2546B  3.664221  3.448133    3.578464     3.666472  3.410824   \n",
       "\n",
       "   conventions  \n",
       "0     2.623977  \n",
       "1     2.704002  \n",
       "2     3.292725  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.to_csv(\"submission.csv\",index=None)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea88ab",
   "metadata": {
    "papermill": {
     "duration": 0.885861,
     "end_time": "2022-11-04T05:29:06.289014",
     "exception": false,
     "start_time": "2022-11-04T05:29:05.403153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d8d24",
   "metadata": {
    "papermill": {
     "duration": 0.886182,
     "end_time": "2022-11-04T05:29:08.180211",
     "exception": false,
     "start_time": "2022-11-04T05:29:07.294029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9511.701413,
   "end_time": "2022-11-04T05:29:12.649004",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-04T02:50:40.947591",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
